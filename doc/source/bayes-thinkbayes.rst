
贝叶斯思维
===========


内容提要：

- 贝叶斯方法是一种常见的利用概率学知识取解决不确定性问题的数学方法

- 本书的主要内容涉及：

    - 建模决策的方法论

    - 建模误差和数值误差怎么取舍

    - 怎样为具体问题建立数学模型

    - 如何抓住问题中的主要矛盾(模型中的关键参数)

    - 优化或验证模型的有效性或者局限性

推荐者序

- 事情的发生“既有必然性，又有偶然性”

- 很多时候，我们不能给出每个人、每一件事的确定结果。但是，当我们观察大量的相同事件后，我们就会发现从一个集体的意义上的规律是存在的。
  而单个事件每次可能得到不同的结果，这些结果一最有可能的结果为中心，服从一定的概率分布。了解这些分布数据，使我们更加容易理解和预期真实世界的多边形。



1.贝叶斯概率
----------------------

本节摘录于《概率论与数理统计教程-第二版(茆诗松 程依明 濮晓龙)》、《高等数理统计-(茆诗松 程依明 濮晓龙)》

    在统计学中有两个大的学派：**频率学派** (也称经典学派) 和 **贝叶斯学派**

1.1 条件概率、贝叶斯公式
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

条件概率的三个公式中：

    - **乘法公式** 是求事件交的概率

    - **全概率公式** 是求一个复杂事件的概率

    - **贝叶斯公式** 是求一个条件概率

条件概率
^^^^^^^^^^^^^^^^^

.. important:: 

    所谓条件概率，它是指在某事件 :math:`B`  发生的条件下，求另一件事 :math:`A`  的概率，
    记为 :math:`P(A|B)`，它与 :math:`P(A)` 是不同的两类概率。 条件概率是两个无条件概率之商，
    这就是条件概率的定义.


- **定义**：设 :math:`A` 与 :math:`B` 是样本空间 :math:`\Omega` 中的两个事件，若 :math:`P(B) > 0`，
    则称下面的公式为“在 :math:`B` 发生下 :math:`A` 的条件概率”，简称**条件概率**。
    
    :math:`P(A|B) = \frac{P(AB)}{P(B)}` 

    - **性质**：条件概率是概率，即若设 :math:`P(B) > 0`，则

        - (1) :math:`P(A|B) \geq 0, A \in \mathcal{F}`

        - (2) :math:`P(\Omega|B) = 1`.

        - (3) 若 :math:`\mathcal{F}` 中的 :math:`A_1, A_2, \cdots, A_n, \cdots` 互不相容，则：

            :math:`P(\bigcup_{n=1}^{\infty}A_{n}|B) = \sum_{n = 1}^{\infty}P(A_{n}|B)`

乘法公式
^^^^^^^^^^^^^^^^

    - **性质**：

        - (1) 若 :math:`P(B)>0`，则

            - :math:`P(AB)=P(B)P(A|B)=P(A)P(B|A)`
        
        - (2) 若 :math:`P(A_1, A_2, \cdots, A_{n-1})>0`，则

            - :math:`P(A_1, A_2, \cdots, A_{n})=P(A_{1})P(A_{2}|A_{1})P(A_{3}|A_{1}A_{2}) \cdots P(A_{n}|A_{1} \cdots A_{n-1})`

全概率公式
^^^^^^^^^^^^^^^^

    - **性质**：设 :math:`B_{1}, B_{2}, \cdots, B_{n}` 为样本空间 :math:`\Omega` 的一个分割，即 :math:`B_{1}, B_{2}, \cdots, B_{n}` 互不相容，
      且 :math:`\bigcup_{i=1}^{n}B_{i} = \Omega`，如果 :math:`P(B_{i})>0,i=1,2,\cdots,n`，则对任一事件 :math:`A` 有

        :math:`P(A)=\sum_{n = 1}^{\infty}P(B_{i})P(A|B_{i})`

    - 全概率公式的最简单形式：

        - 假如 :math:`0<P(B)<1`，则

            - :math:`P(A) = P(B)P(A|B) + P(\bar{B})P(A|\bar{B})`
        
    - 条件 :math:`B_{1}, B_{2}, \cdots, B_{n}` 为样本空间的一个分割，可改成 :math:`B_{1}, B_{2}, \cdots, B_{n}` 互不相容，
      且 :math:`A \subset \bigcup_{i=1}^{n}B_{i}`，性质中的定理仍然成立。

贝叶斯公式
^^^^^^^^^^^^^^^^^

    - **性质**：设 :math:`B_{1}, B_{2}, \cdots, B_{n}` 为样本空间 :math:`\Omega` 的一个分割，即 :math:`B_{1}, B_{2}, \cdots, B_{n}` 互不相容，
      且 :math:`\bigcup_{i=1}^{n}B_{i} = \Omega`，如果 :math:`P(A) > 0, P(B_{i})>0,i=1,2,\cdots,n`，则

        :math:`P(B_{i}|A)=\frac{P(B_{i})P(A|B_{i})}{\sum_{j=1}^{n}P(B_{j})P(A|B_{j})}, i=1,2,\cdots,n.`

.. note:: 

    - 在贝叶斯公式中，如果称 :math:`P(B_{i})` 为 :math:`B_{i}` 的 **先验概率**，称 :math:`P(B_{i}|A)>0` 为 :math:`B_{i}` 的 **后验概率**，
      则贝叶斯公式是专门用于计算后验概率的，也就是通过 :math:`A` 的发生这个新信息，来对 :math:`B_{i}` 的概率作出修正.

        - :math:`P(B_{i}|A)=P(B_{i}) \cdot \frac{P(A|B_{i})}{\sum_{j=1}^{n}P(B_{j})P(A|B_{j})}, i=1,2,\cdots,n.`



1.2 贝叶斯估计
~~~~~~~~~~~~~~~~~~~~~

1.2.1 统计推断的基础
^^^^^^^^^^^^^^^^^^^^

    - 经典统计学派对统计推断的规定如下：

        - 统计推断是根据样本信息对总体分布或总体的特征数进行推断，这里的统计推断使用到了两种信息：**总体信息** 和 **样本信息**.

    - 贝叶斯学派则认为：

        - 统计推断除了总体信息、样本信息以外，还应该使用第三种信息：**先验信息**.

总体信息：
''''''''''''

    - 总体信息即总体分布或总体所属分布族提供的信息.


样本信息：
''''''''''''

    - 样本信息即抽取样本所得观测值提供的信息.

先验信息：
''''''''''''

    - 如果把抽取样本看作做一次试验，则样本信息就是试验中得到的信息. 
      实际上，人们在试验之前要对要做的的问题在经验上和资料上总是有所了解的，
      这些信息对统计推断是有益的

    - 先验信息即是抽样(试验)之前有关统计问题的一些信息

    - 一般来说，先验信息来源于经验和历史资料

贝叶斯统计学：
'''''''''''''''''''

基于上述三种信息进行统计推断的统计学称为 **贝叶斯统计学**. 它与经典统计学的差别就在于是否利用先验信息。

贝叶斯统计在重视使用总体信息和样本信息的同时，还注意先验信息的收集、挖掘和加工，使它数量化，形成先验分布，参加到
统计推断中来，以提高统计推断的质量。

忽视先验信息的利用，有时是一种浪费，有时还会导出不合理的结论。

贝叶斯学派的基本观点是：任一未知量 :math:`\theta` 都可看作随机变量，可用一个概率分布区描述，这个分布称为先验分布；

在


1.2.2 贝叶斯公式的密度函数形式
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^




1.2.3 贝叶斯估计
^^^^^^^^^^^^^^^^^^^^

由后验分布 :math:`\pi(\theta|X)` 估计 :math:`\theta` 有三种常用的方法：

    - 使用后验分布的密度函数最大值点作为 :math:`\theta` 的点估计的最大后验估计.

    - 使用后验分布的中位数作为 :math:`\theta` 的点估计的后验中位数估计.

    - 使用后验分布的均值作为 :math:`\theta` 的点估计的后验期望估计. 用的最多的是后延期望估计，它一般称为贝叶斯估计，记为 :math:`\hat{\theta}_{B}`.

1.2.4 共轭先验分布
^^^^^^^^^^^^^^^^^^^^

从贝叶斯公式可以看出，整个贝叶斯统计推断只要先验分布确定后就没有理论上的困难. 关于先验分布的确定有多种途径，最常用的先验分布类为 **共轭鲜艳分布**。

    - **定义**：设 :math:`\theta` 是总体分布 :math:`p(x;\theta)` 中的参数， :math:`\pi(\theta)` 是其先验分布，若
      对任意来自 :math:`p(x;\theta)` 的样本观测值得到的后验分布 :math:`\pi(\theta|x)` 与 :math:`\pi(\theta)` 属于同一个分布族，
      则称该分布是 :math:`\theta` 的共轭鲜艳分布(族).



1.5 统计决策理论与 Bayes 分析
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- 经典统计学

    - (待补充)

- 统计决策理论

    - **统计决策理论** 是著名统计学家 A.Wald(1902-1950) 在 20 世纪 40 年代建立起来的，
      它与经典统计学的差别在于是否涉及后果。经典统计学着重在推断上
      而不考虑在何处和效益如何。而统计决策理论引入*损失函数*，用来度量效益大小，
      评价统计推断结果的优劣。

- Bayes 分析、非决策的 Bayes 分析、Bayes 决策分析

    - **Bayes 分析** 是英国学者 T.Bayes(1702-1761) 首先提出，在 20 世纪后半叶发展迅速。
      它与经典统计学的差别在于 **是否使用先验信息(经验与历史资料)**。经典统计学只用样本信息，而
      Bayes 分析把先验信息与样本信息结合起来用于推断之中，形成 **非决策的 Bayes 分析**。
      若再使用后果信息，就形成 **Bayes 决策分析**。


1.5.1 统计决策问题和损失函数
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


1.5.2 决策函数和风险函数
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


1.5.3 Bayes 决策准则和 Bayes 分析
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



2.贝叶斯思维
----------------------

使用 Python 代码实现的 Bayes 方法不是数学，离散近似而不是连续数学，
结果就是原本需要积分的地方变成了求和，概率分布的大多数操作变成了简单的循环。

书信息：

    - https://greenteapress.com/wp/think-bayes/

    - https://github.com/wangzhefeng/ThinkBayes2

    - https://github.com/rlabbe/ThinkBayes

环境配置：

    .. code-block:: shell

        $ git clone git@github.com:wangzhefeng/ThinkBayes2.git
        $ cd ThinkBayes2
        $ pip install .
        $ python -m pip install numpy scipy matplotlib jupyter pandas jupyterlab
        $ python install_test.py


2.1 贝叶斯定理
~~~~~~~~~~~~~~~~~~~~~



2.2 统计计算
~~~~~~~~~~~~~~~~~~~~~~



2.3 估计
~~~~~~~~~~~~~~~~~~~~~~




