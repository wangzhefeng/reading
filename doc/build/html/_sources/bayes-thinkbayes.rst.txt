
贝叶斯思维
=============

内容提要：

- 贝叶斯方法是一种常见的利用概率学知识取解决不确定性问题的数学方法

- 本书的主要内容涉及：

    - 建模决策的方法论

    - 建模误差和数值误差怎么取舍

    - 怎样为具体问题建立数学模型

    - 如何抓住问题中的主要矛盾(模型中的关键参数)

    - 优化或验证模型的有效性或者局限性

推荐者序

- 事情的发生“既有必然性，又有偶然性”

- 很多时候，我们不能给出每个人、每一件事的确定结果。但是，当我们观察大量的相同事件后，我们就会发现从一个集体的意义上的规律是存在的。
  而单个事件每次可能得到不同的结果，这些结果一最有可能的结果为中心，服从一定的概率分布。了解这些分布数据，使我们更加容易理解和预期真实世界的多边形。



1.贝叶斯概率
----------------------

本节摘录于《概率论与数理统计教程-第二版(茆诗松 程依明 濮晓龙)》、《高等数理统计-(茆诗松 程依明 濮晓龙)》

    在统计学中有两个大的学派：**频率学派** (也称经典学派) 和 **贝叶斯学派**

1.1 条件概率、贝叶斯公式
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

条件概率的三个公式中：

    - **乘法公式** 是求事件交的概率

    - **全概率公式** 是求一个复杂事件的概率

    - **贝叶斯公式** 是求一个条件概率

条件概率
^^^^^^^^^^^^^^^^^

.. important:: 

    所谓条件概率，它是指在某事件 :math:`B`  发生的条件下，求另一件事 :math:`A`  的概率，
    记为 :math:`P(A|B)`，它与 :math:`P(A)` 是不同的两类概率。 条件概率是两个无条件概率之商，
    这就是条件概率的定义.


- **定义**：设 :math:`A` 与 :math:`B` 是样本空间 :math:`\Omega` 中的两个事件，若 :math:`P(B) > 0`，
    则称下面的公式为“在 :math:`B` 发生下 :math:`A` 的条件概率”，简称**条件概率**。
    
    :math:`P(A|B) = \frac{P(AB)}{P(B)}` 

    - **性质**：条件概率是概率，即若设 :math:`P(B) > 0`，则

        - (1) :math:`P(A|B) \geq 0, A \in \mathcal{F}`

        - (2) :math:`P(\Omega|B) = 1`.

        - (3) 若 :math:`\mathcal{F}` 中的 :math:`A_1, A_2, \cdots, A_n, \cdots` 互不相容，则：

            :math:`P(\bigcup_{n=1}^{\infty}A_{n}|B) = \sum_{n = 1}^{\infty}P(A_{n}|B)`

乘法公式
^^^^^^^^^^^^^^^^

    - **性质**：

        - (1) 若 :math:`P(B)>0`，则

            - :math:`P(AB)=P(B)P(A|B)=P(A)P(B|A)`
        
        - (2) 若 :math:`P(A_1, A_2, \cdots, A_{n-1})>0`，则

            - :math:`P(A_1, A_2, \cdots, A_{n})=P(A_{1})P(A_{2}|A_{1})P(A_{3}|A_{1}A_{2}) \cdots P(A_{n}|A_{1} \cdots A_{n-1})`

全概率公式
^^^^^^^^^^^^^^^^

    - **性质**：设 :math:`B_{1}, B_{2}, \cdots, B_{n}` 为样本空间 :math:`\Omega` 的一个分割，即 :math:`B_{1}, B_{2}, \cdots, B_{n}` 互不相容，
      且 :math:`\bigcup_{i=1}^{n}B_{i} = \Omega`，如果 :math:`P(B_{i})>0,i=1,2,\cdots,n`，则对任一事件 :math:`A` 有

        :math:`P(A)=\sum_{n = 1}^{\infty}P(B_{i})P(A|B_{i})`

    - 全概率公式的最简单形式：

        - 假如 :math:`0<P(B)<1`，则

            - :math:`P(A) = P(B)P(A|B) + P(\bar{B})P(A|\bar{B})`
        
    - 条件 :math:`B_{1}, B_{2}, \cdots, B_{n}` 为样本空间的一个分割，可改成 :math:`B_{1}, B_{2}, \cdots, B_{n}` 互不相容，
      且 :math:`A \subset \bigcup_{i=1}^{n}B_{i}`，性质中的定理仍然成立。

贝叶斯公式
^^^^^^^^^^^^^^^^^

    - **性质**：设 :math:`B_{1}, B_{2}, \cdots, B_{n}` 为样本空间 :math:`\Omega` 的一个分割，即 :math:`B_{1}, B_{2}, \cdots, B_{n}` 互不相容，
      且 :math:`\bigcup_{i=1}^{n}B_{i} = \Omega`，如果 :math:`P(A) > 0, P(B_{i})>0,i=1,2,\cdots,n`，则

        :math:`P(B_{i}|A)=\frac{P(B_{i})P(A|B_{i})}{\sum_{j=1}^{n}P(B_{j})P(A|B_{j})}, i=1,2,\cdots,n.`

.. note:: 

    - 在贝叶斯公式中，如果称 :math:`P(B_{i})` 为 :math:`B_{i}` 的 **先验概率**，称 :math:`P(B_{i}|A)>0` 为 :math:`B_{i}` 的 **后验概率**，
      则贝叶斯公式是专门用于计算后验概率的，也就是通过 :math:`A` 的发生这个新信息，来对 :math:`B_{i}` 的概率作出修正.

        - :math:`P(B_{i}|A)=P(B_{i}) \cdot \frac{P(A|B_{i})}{\sum_{j=1}^{n}P(B_{j})P(A|B_{j})}, i=1,2,\cdots,n.`



1.2 贝叶斯估计
~~~~~~~~~~~~~~~~~~~~~

1.2.1 统计推断的基础
^^^^^^^^^^^^^^^^^^^^

    - 经典统计学派对统计推断的规定如下：

        - 统计推断是根据样本信息对总体分布或总体的特征数进行推断，这里的统计推断使用到了两种信息：**总体信息** 和 **样本信息**.

    - 贝叶斯学派则认为：

        - 统计推断除了总体信息、样本信息以外，还应该使用第三种信息：**先验信息**.

总体信息：
''''''''''''

    - 总体信息即总体分布或总体所属分布族提供的信息.


样本信息：
''''''''''''

    - 样本信息即抽取样本所得观测值提供的信息.

先验信息：
''''''''''''

    - 如果把抽取样本看作做一次试验，则样本信息就是试验中得到的信息. 
      实际上，人们在试验之前要对要做的的问题在经验上和资料上总是有所了解的，
      这些信息对统计推断是有益的

    - 先验信息即是抽样(试验)之前有关统计问题的一些信息

    - 一般来说，先验信息来源于经验和历史资料

贝叶斯统计学：
'''''''''''''''''''

基于上述三种信息进行统计推断的统计学称为 **贝叶斯统计学**. 它与经典统计学的差别就在于是否利用先验信息。

贝叶斯统计在重视使用总体信息和样本信息的同时，还注意先验信息的收集、挖掘和加工，使它数量化，形成先验分布，参加到
统计推断中来，以提高统计推断的质量。

忽视先验信息的利用，有时是一种浪费，有时还会导出不合理的结论。

贝叶斯学派的基本观点是：任一未知量 :math:`\theta` 都可看作随机变量，可用一个概率分布区描述，这个分布称为先验分布；

在


1.2.2 贝叶斯公式的密度函数形式
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^




1.2.3 贝叶斯估计
^^^^^^^^^^^^^^^^^^^^

由后验分布 :math:`\pi(\theta|X)` 估计 :math:`\theta` 有三种常用的方法：

    - 使用后验分布的密度函数最大值点作为 :math:`\theta` 的点估计的最大后验估计.

    - 使用后验分布的中位数作为 :math:`\theta` 的点估计的后验中位数估计.

    - 使用后验分布的均值作为 :math:`\theta` 的点估计的后验期望估计. 用的最多的是后延期望估计，它一般称为贝叶斯估计，记为 :math:`\hat{\theta}_{B}`.

1.2.4 共轭先验分布
^^^^^^^^^^^^^^^^^^^^

从贝叶斯公式可以看出，整个贝叶斯统计推断只要先验分布确定后就没有理论上的困难. 关于先验分布的确定有多种途径，最常用的先验分布类为 **共轭鲜艳分布**。

    - **定义**：设 :math:`\theta` 是总体分布 :math:`p(x;\theta)` 中的参数， :math:`\pi(\theta)` 是其先验分布，若
      对任意来自 :math:`p(x;\theta)` 的样本观测值得到的后验分布 :math:`\pi(\theta|x)` 与 :math:`\pi(\theta)` 属于同一个分布族，
      则称该分布是 :math:`\theta` 的共轭鲜艳分布(族).



1.5 统计决策理论与 Bayes 分析
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- 经典统计学

    - (待补充)

- 统计决策理论

    - **统计决策理论** 是著名统计学家 A.Wald(1902-1950) 在 20 世纪 40 年代建立起来的，
      它与经典统计学的差别在于是否涉及后果。经典统计学着重在推断上
      而不考虑在何处和效益如何。而统计决策理论引入*损失函数*，用来度量效益大小，
      评价统计推断结果的优劣。

- Bayes 分析、非决策的 Bayes 分析、Bayes 决策分析

    - **Bayes 分析** 是英国学者 T.Bayes(1702-1761) 首先提出，在 20 世纪后半叶发展迅速。
      它与经典统计学的差别在于 **是否使用先验信息(经验与历史资料)**。经典统计学只用样本信息，而
      Bayes 分析把先验信息与样本信息结合起来用于推断之中，形成 **非决策的 Bayes 分析**。
      若再使用后果信息，就形成 **Bayes 决策分析**。


1.5.1 统计决策问题和损失函数
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


1.5.2 决策函数和风险函数
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


1.5.3 Bayes 决策准则和 Bayes 分析
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



2.贝叶斯思维
----------------------

使用 Python 代码实现的 Bayes 方法不是数学，离散近似而不是连续数学，
结果就是原本需要积分的地方变成了求和，概率分布的大多数操作变成了简单的循环。

2.1 建模和近似
~~~~~~~~~~~~~~~~~~~

在应用任何分析方法前，必须决定真实世界中的哪些部分可以被包括进模型，而哪些细节可以被抽象掉。

在解决问题的过程中，明确建模过程作为其中一部分是重要的，因为这会提醒我们考虑建模误差(也就是建模当中简化和假设带来的误差)。

本书中很多方法都基于离散分布，这让一些人担心数值误差，但对于真实世界的问题，数值误差几乎从来都小于建模误差。
再者，离散方法总能允许较好的建模选择，我宁愿要一个近似的良好的模型也不要一个精确但却糟糕的模型。

从另一个角度看，连续方法常在性能上有优势，比如能以常数时间复杂度的解法替换掉线性或者平方时间复杂度的解法。

总的来说，推荐这些步骤的一个通用流程如下：

    - 1.当研究问题时，以一个简化模型开始，并以清晰、好理解、实证无误的代码实现它。注意力集中在好的建模决策而不是优化上

    - 2.一旦简化模型有效，再找到最大错误来源。这可能需要增加离散近似过程当中值的数量，或者增加蒙特卡洛方法中的迭代数，或者增加模型细节。

    - 3.如果对你的应用而言性能就已经足够了，则没必要优化。但如果要做，有两个方向可以考虑：评估你的代码以寻找优化空间，例如，如果你缓存了
      前面的计算结果，你也许能避免重复冗余的计算；或者可以去发现找到计算捷径的分析方法。


2.2 代码利用
~~~~~~~~~~~~~~~~~~~

书信息：

    - https://greenteapress.com/wp/think-bayes/

    - https://github.com/wangzhefeng/ThinkBayes2

    - https://github.com/rlabbe/ThinkBayes

环境配置：

    .. code-block:: shell

        $ git clone git@github.com:wangzhefeng/ThinkBayes2.git
        $ cd ThinkBayes2
        $ pip install .
        $ python -m pip install numpy scipy matplotlib jupyter pandas jupyterlab
        $ python install_test.py


2.3 贝叶斯定理
~~~~~~~~~~~~~~~~~~~~~

基本概念
^^^^^^^^^^^^^^^^

    - 概率

    - 条件概率

    - 联合概率


贝叶斯定理
^^^^^^^^^^^^^^^^

- 联合概率满足乘法交换律：

    - :math:`P(AB)  = P(BA)` 

- 乘法公式：

    - :math:`P(AB) = P(A)P(B|A)`

    - :math:`P(BA) = P(B)P(A|B)`

    - :math:`P(A)P(B|A) = P(B)P(A|B)` 

- 贝叶斯定理：

    - :math:`P(A|B) = \frac{P(A)P(B|A)}{P(B)}` 

.. note:: 

    - 对于涉及条件概率的很多问题，贝叶斯定理提供了一个分而治之的策略。

    - 如果 :math:`P(A|B)` 难以计算，或难以用实验衡量，可以检查计算贝叶斯定理中的其他项是否更容易，如 :math:`P(B|A)`，
      :math:`P(A)` 和 :math:`P(B)`.  

2.4 统计计算
~~~~~~~~~~~~~~~~~~~~~~

2.4.1 分布
^^^^^^^^^^^^^^^^^^^^^^^

在统计学中，分布是一组值及其对应的概率。

使用示例：

    - 建立一个 ``Pmf`` 来表示六面筛骰子的结果分布

        .. code-block:: python

            from thinkbayes2 import Pmf

            pmf = Pmf()
            for x in [1, 2, 3, 4, 5, 6]:
                pmf.Set(x, 1/6.0)
            
            print(pmf.Prob(1))
            print(pmf.Prob(7))


    - 计算每个单词在一个词序列中出现的次数

        .. code-block:: python
        
            from thinkbayes2 import Pmf
            
            pmf = Pmf()
            word_list = ["the", "wang", "zhe", "feng", "the"]
            for word in word_list:
                pmf.Incr(word, 1)
            
            print(pmf.Prob("the"))
            print(pmf.Prob("the") / pmf.Normalize())

2.4.2 曲奇饼问题
^^^^^^^^^^^^^^^^^^^^^^^

假设是事件 :math:`B_1` 和 :math:`B_2`

    .. code-block:: python

        from thinkbayes2 import Pmf

        # 先验分布
        pmf = Pmf()
        pmf.Set("Bowl1", 0.5)
        pmf.Set("Bowl2", 0.5)

        # 更新基于数据(拿到一块香草曲奇饼) 后的分布，将先验分别乘以对应的似然度
        pmf.Mult("Bowl1", 0.75)
        pmf.Mult("Bowl2", 0.5)

        # 分布归一化
        pmf.Normalize()

        # 后验分布
        print(pmf.Prob("Bowl1"))
        print(pmf.Prob("Bowl2"))

- 贝叶斯框架:

    .. code-block:: python

        from thinkbayes2 import Pmf
        class Cookie(Pmf):
            def __init__(self, hypos):
                Pmf.__init__(self)
                for hypo in hypos:
                    self.Set(hypo, 1)
                self.Normalize()
            
            def Update(self, data):
                for hypo in self.Values():
                    like = self.Likelihood(data, hypo)
                    self.Mult(hypo, like)
                self.Normalize()
            
            mixes = {
                "Bowl1": dict(
                    vanilla = 0.75, chocolate = 0.25
                ),
                "Bowl2": dict(
                    vanilla = 0.5， chocolate = 0.5
                ),
            }
            def Likelihood(self, data, hypo):
                mix = self.mixes(hypo)
                like = mix[data]
                return like

        hypos = ["Bowl1", "Bowl2"]
        pmf = Cookie(hypos)

        # 更新
        pmf.Update("vanilla")

        # 打印每个假设的后验概率
        for hypo, prob in pmf.Items():
            print(hypo, prob)
        # 推广到从同一个碗取不只一个曲奇饼(带替换)的情形
        dataset = ["vanilla", "chocolate", "vanilla"]
        for data in dataset:
            pmf.Update(data)

2.4.3 Monty Hall 难题
^^^^^^^^^^^^^^^^^^^^^^^^^

Monty Hall 类实现：

    .. code-block:: python

        class Monty(Pmf):

            def __init__(self, hypos):
                Pmf.__init__(self)
                for hypo in hypos:
                    self.Set(hypo, 1)
                self.Normalize()

            def Update(self, data):
                for hypo in self.Values():
                    like = self.Likelihood(data, hypo)
                    self.Mult(hypo, like)
                self.Normalize()

            def Likelihood(self, data, hypo):
                if hypo == data:
                    return 0
                elif hypo == "A":
                    return 0.5
                else:
                    return 1

        hypos = "ABC"
        pmf = Monty(hypos)
        data = "B"
        pmf.Update(data)

        for hypo, prob in pmf.Items():
            print(hypo, prob)

封装框架:

    .. code-block:: python

        class Suite(Pmf):
            """代表一套假设及其概率"""

            def __init__(self, hypo = tuple()):
                """初始化分配"""
                pass
            
            def Update(self, data):
                """更新基于该数据的每个假设"""
                pass

            def Print(self):
                """打印出假设和它们的概率"""
                pass

    .. code-block:: python

        from thinkbayes2 import Suite

        class Monty(Suite):
            
            def Likelihood(self, data, hypo):
                if hypo == data:
                    return 0
                elif hypo == "A":
                    return 0.5
                else:
                    return 1
        
        suite = Monty("ABC")
        suite.Update("B")
        suite.Print()

2.4.4 M&M 豆问题
^^^^^^^^^^^^^^^^^^^^^^

    .. code-block:: python

        mix94 = dict(brown = 30, yellow = 20, red = 20, green = 10, orange = 10, tan = 10)
        mix96 = dict(blue = 24, green = 20, orange = 16, yellow = 14, red = 13, brown = 13)
        hypoA = dict(bag1 = mix94, bag2 = mix96)
        hypoB = dict(bag1 = mix94, bag2 = mix94)

        hypotheses = dict(A = hypoA, B = hypoB)

        class M_and_M(Suite):

            def Likelihood(self, data, hypo):
                bag, color = data
                mix = self.hypotheses[hypo][bag]
                like = mix[color]
                return like

        suite = M_and_M("AB")
        suite.Update(("bag1", "yellow"))
        suite.Update(("bag2", "green"))
        suite.Print()

.. note:: 

    - Suite 是一个抽象类(abstract type)，这意味着它定义了 Suite 应该有额接口，但并不提供完整的实现. Suite 接口包括了 Update 和 Likelihood 方法，但只提供了 Update 的实现，没有 Likelihood 的实现。

    - 具体类(concrerte type)是继承自抽象父类的类，而且提供了缺失方法的实现。


2.5 估计
~~~~~~~~~~~~~~~~~~~~~~




